import chalk from 'chalk';
import modelsJsonData from './_gemoni-json/models.json';

// é…ç½®å¸¸é‡
const CONFIG = {
  PORT_RANGE: { min: 3000, max: 9999 },
  STREAM_DELAY: 100,
  MAX_PORT_RETRIES: 100,
} as const;

const encoder = new TextEncoder();

// å“åº”å¤´é…ç½®
const HEADERS = {
  BASE: {
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive',
    'x-content-type-options': 'nosniff',
  },
  JSON: {
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive',
    'x-content-type-options': 'nosniff',
    'Content-Type': 'application/json; charset=utf-8',
  },
  STREAM: {
    'Cache-Control': 'no-cache',
    Connection: 'keep-alive',
    'x-content-type-options': 'nosniff',
    'Content-Type': 'text/event-stream; charset=utf-8',
  },
} as const;

// Mock å“åº”æ•°æ®
const MOCK_DATA = {
  GEMINI_STREAM: [
    ...Array.from({ length: 5 }, (_, i) => ({
      candidates: [
        { content: { parts: [{ text: `MOCK_${i + 1}\n` }], role: 'model' } },
      ],
      usageMetadata: {
        promptTokenCount: 9,
        totalTokenCount: 9,
        promptTokensDetails: [{ modality: 'TEXT', tokenCount: 9 }],
      },
      modelVersion: 'gemini-1.5-flash',
    })),
    {
      candidates: [
        {
          content: { parts: [{ text: 'MOCK_DONE\n' }], role: 'model' },
          finishReason: 'STOP',
        },
      ],
      usageMetadata: {
        promptTokenCount: 9,
        candidatesTokenCount: 15,
        totalTokenCount: 24,
        promptTokensDetails: [{ modality: 'TEXT', tokenCount: 9 }],
        candidatesTokensDetails: [{ modality: 'TEXT', tokenCount: 15 }],
      },
      modelVersion: 'gemini-1.5-flash',
    },
  ],
  NON_STREAM: {
    candidates: [
      {
        content: { parts: [{ text: 'MOCK_DONE\n' }], role: 'model' },
        finishReason: 'STOP',
      },
    ],
    usageMetadata: {
      promptTokenCount: 9,
      candidatesTokenCount: 15,
      totalTokenCount: 24,
      promptTokensDetails: [{ modality: 'TEXT', tokenCount: 9 }],
      candidatesTokensDetails: [{ modality: 'TEXT', tokenCount: 15 }],
    },
    modelVersion: 'gemini-1.5-flash',
  },
  // OpenAI å…¼å®¹æ•°æ®
  OPENAI_STREAM: [
    {
      choices: [
        { delta: { content: 'MOCK_1\n', role: 'assistant' }, index: 0 },
      ],
      created: 1745680998,
      model: 'gemini-2.0-flash',
      object: 'chat.completion.chunk',
    },
    {
      choices: [
        { delta: { content: 'MOCK_2\n', role: 'assistant' }, index: 0 },
      ],
      created: 1745680998,
      model: 'gemini-2.0-flash',
      object: 'chat.completion.chunk',
    },
    {
      choices: [
        { delta: { content: 'MOCK_3\n', role: 'assistant' }, index: 0 },
      ],
      created: 1745680998,
      model: 'gemini-2.0-flash',
      object: 'chat.completion.chunk',
    },
    {
      choices: [
        {
          delta: { content: 'MOCK_DONE\n', role: 'assistant' },
          finish_reason: 'stop',
          index: 0,
        },
      ],
      created: 1745680998,
      model: 'gemini-2.0-flash',
      object: 'chat.completion.chunk',
    },
  ],
  OPENAI_MODELS: {
    object: 'list',
    data: [
      { id: 'models/chat-bison-001', object: 'model', owned_by: 'google' },
      { id: 'models/text-bison-001', object: 'model', owned_by: 'google' },
      { id: 'models/gemini-1.5-flash', object: 'model', owned_by: 'google' },
      { id: 'models/gemini-1.5-pro', object: 'model', owned_by: 'google' },
      { id: 'models/gemini-2.0-flash', object: 'model', owned_by: 'google' },
    ],
  },
  OPENAI_MODEL_RETRIEVE: {
    id: 'models/gemini-2.0-flash',
    object: 'model',
    owned_by: 'google',
  },
  OPENAI_COMPLETION: {
    choices: [
      {
        finish_reason: 'stop',
        index: 0,
        message: {
          content: 'Hello! How can I help you today?\n',
          role: 'assistant',
        },
      },
    ],
    created: 1745682706,
    model: 'gemini-2.0-flash',
    object: 'chat.completion',
    usage: {
      completion_tokens: 8,
      prompt_tokens: 10,
      total_tokens: 18,
    },
  },
} as const;

// å·¥å…·å‡½æ•°
function createJsonResponse(data: any): Response {
  return new Response(JSON.stringify(data), { headers: HEADERS.JSON });
}

function createStreamResponse(stream: ReadableStream<Uint8Array>): Response {
  return new Response(stream, { headers: HEADERS.STREAM });
}

function createStreamData(data: any): string {
  return `data: ${JSON.stringify(data)}\n\n`;
}

function logRequest(
  method: string,
  path: string,
  query?: URLSearchParams,
): void {
  const queryStr = query?.toString() ? `?${query.toString()}` : '';
  console.log(chalk.blue(`ğŸ“¥ [${method}] ${path}${queryStr}`));
}

// è·¯ç”±å¤„ç†å™¨ç±»
class RouteHandler {
  private routes: Map<string, () => Response | Promise<Response>>;

  constructor() {
    this.routes = new Map();
    this.setupRoutes();
  }

  private setupRoutes(): void {
    // æ¨¡å‹åˆ—è¡¨è·¯ç”±
    this.routes.set('/v1beta/models', () => {
      return createJsonResponse(modelsJsonData);
    });

    // ç”Ÿæˆå†…å®¹è·¯ç”±ï¼ˆæµå¼å’Œéæµå¼ï¼‰
    this.routes.set('/v1beta/models/gemini-1.5-flash:generateContent', () => {
      return createJsonResponse(MOCK_DATA.NON_STREAM);
    });

    this.routes.set(
      '/v1beta/models/gemini-1.5-flash:streamGenerateContent',
      () => {
        return this.createStreamingResponse();
      },
    );

    this.routes.set('/v1beta/models/gemini-1.5-pro:generateContent', () => {
      return createJsonResponse(MOCK_DATA.NON_STREAM);
    });

    this.routes.set(
      '/v1beta/models/gemini-1.5-pro:streamGenerateContent',
      () => {
        return this.createStreamingResponse();
      },
    );

    // OpenAI å…¼å®¹è·¯ç”±
    this.routes.set('/v1beta/openai/models', () => {
      return createJsonResponse(MOCK_DATA.OPENAI_MODELS);
    });
  }

  private createStreamingResponse(): Response {
    const stream = new ReadableStream({
      async start(controller) {
        try {
          for (const chunk of MOCK_DATA.GEMINI_STREAM) {
            const data = createStreamData(chunk);
            controller.enqueue(encoder.encode(data));
            await new Promise((resolve) =>
              setTimeout(resolve, CONFIG.STREAM_DELAY),
            );
          }
        } catch (error) {
          console.error('æµå¼å“åº”é”™è¯¯:', error);
        } finally {
          controller.close();
        }
      },
    });

    return createStreamResponse(stream);
  }

  private createOpenAIStreamingResponse(): Response {
    const stream = new ReadableStream({
      async start(controller) {
        try {
          for (const chunk of MOCK_DATA.OPENAI_STREAM) {
            const data = createStreamData(chunk);
            controller.enqueue(encoder.encode(data));
            await new Promise((resolve) =>
              setTimeout(resolve, CONFIG.STREAM_DELAY),
            );
          }
        } catch (error) {
          console.error('OpenAIæµå¼å“åº”é”™è¯¯:', error);
        } finally {
          controller.close();
        }
      },
    });

    return createStreamResponse(stream);
  }

  async handle(req: Request): Promise<Response> {
    try {
      const url = new URL(req.url);
      const path = url.pathname;
      const method = req.method;

      logRequest(method, path, url.searchParams);

      // é¦–å…ˆæ£€æŸ¥å›ºå®šè·¯ç”±
      const fixedHandler = this.routes.get(path);
      if (fixedHandler) {
        return await fixedHandler();
      }

      // å¤„ç†åŠ¨æ€è·¯ç”±å’Œæ¨¡å¼åŒ¹é…

      // Gemini é€šç”¨ç”Ÿæˆå†…å®¹è·¯ç”±: /v1beta/models/{model_name}:generateContent
      if (
        path.includes('/v1beta/models/') &&
        path.endsWith(':generateContent') &&
        method === 'POST'
      ) {
        const modelName = path
          .split('/v1beta/models/')[1]
          .split(':generateContent')[0];
        console.log(
          chalk.green.bold(
            `ğŸ­ğŸ“ [MOCK] æ¨¡æ‹Ÿ Gemini éæµå¼å“åº” (${modelName}) ğŸ“ğŸ­`,
          ),
        );
        return createJsonResponse(MOCK_DATA.NON_STREAM);
      }

      // Gemini é€šç”¨æµå¼ç”Ÿæˆå†…å®¹è·¯ç”±: /v1beta/models/{model_name}:streamGenerateContent
      if (
        path.includes('/v1beta/models/') &&
        path.endsWith(':streamGenerateContent') &&
        method === 'POST'
      ) {
        const modelName = path
          .split('/v1beta/models/')[1]
          .split(':streamGenerateContent')[0];
        console.log(
          chalk.green.bold(
            `ğŸ­ğŸŒŠ [MOCK] æ¨¡æ‹Ÿ Gemini æµå¼å“åº” (${modelName}) ğŸŒŠğŸ­`,
          ),
        );
        return this.createStreamingResponse();
      }

      // OpenAI æ¨¡å‹æ£€ç´¢: /v1beta/openai/models/{model_id}
      if (path.startsWith('/v1beta/openai/models/') && method === 'GET') {
        const modelId = path.split('/').pop();
        console.log(
          chalk.green.bold(`ğŸ­ğŸ” [MOCK] æ¨¡æ‹Ÿ OpenAI æ£€ç´¢æ¨¡å‹: ${modelId} ğŸ”ğŸ­`),
        );
        return createJsonResponse({
          ...MOCK_DATA.OPENAI_MODEL_RETRIEVE,
          id: `models/${modelId}`,
        });
      }

      // OpenAI èŠå¤©å®Œæˆ: /v1beta/openai/chat/completions
      if (path === '/v1beta/openai/chat/completions' && method === 'POST') {
        const requestBody = await req.json();
        const isStream = requestBody.stream === true;

        if (isStream) {
          console.log(
            chalk.yellow.bold(`ğŸ­ğŸŒŠ [MOCK] æ¨¡æ‹Ÿ OpenAI æµå¼å“åº” ğŸŒŠğŸ­`),
          );
          return this.createOpenAIStreamingResponse();
        } else {
          console.log(
            chalk.yellow.bold(`ğŸ­ğŸ“ [MOCK] æ¨¡æ‹Ÿ OpenAI éæµå¼å“åº” ğŸ“ğŸ­`),
          );
          return createJsonResponse(MOCK_DATA.OPENAI_COMPLETION);
        }
      }

      // é»˜è®¤å“åº”
      return createJsonResponse({
        error: 'Not Found',
        path,
        available_routes: Array.from(this.routes.keys()).concat([
          '/v1beta/openai/models/{model_id}',
          '/v1beta/openai/chat/completions',
        ]),
      });
    } catch (error) {
      console.error('è¯·æ±‚å¤„ç†é”™è¯¯:', error);
      return createJsonResponse({
        error: 'Internal Server Error',
        message: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
      });
    }
  }
}

// ç«¯å£æŸ¥æ‰¾å‡½æ•°
async function findAvailablePort(): Promise<number> {
  const { min, max } = CONFIG.PORT_RANGE;
  let attempts = 0;

  while (attempts < CONFIG.MAX_PORT_RETRIES) {
    const port = Math.floor(Math.random() * (max - min + 1)) + min;

    try {
      // å°è¯•åœ¨ç«¯å£ä¸Šåˆ›å»ºæœåŠ¡å™¨æ¥æµ‹è¯•å¯ç”¨æ€§
      const testServer = Bun.serve({
        port,
        fetch() {
          return new Response('test');
        },
      });

      testServer.stop();
      return port;
    } catch {
      attempts++;
    }
  }

  throw new Error('æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£');
}

// ä¸»å‡½æ•°
export async function startMockServer(): Promise<number> {
  const port = await findAvailablePort();
  const routeHandler = new RouteHandler();

  console.log(chalk.green.bold(`ğŸ­ğŸš€ [MOCK-SERVER] å¯åŠ¨åœ¨ç«¯å£ ${port} ğŸš€ğŸ­`));

  Bun.serve({
    port,
    async fetch(req) {
      return await routeHandler.handle(req);
    },
  });

  return port;
}

// å¦‚æœä½œä¸ºä¸»æ¨¡å—ç›´æ¥è¿è¡Œï¼Œå¯åŠ¨æœåŠ¡å™¨è¿›è¡Œæµ‹è¯•
if (import.meta.main) {
  console.log('ğŸ§ª ç›´æ¥è¿è¡Œä¼˜åŒ–åçš„MockServerè¿›è¡Œæµ‹è¯•...');

  try {
    const port = await startMockServer();
    console.log(`âœ… MockServerå¯åŠ¨æˆåŠŸï¼Œç«¯å£: ${port}`);

    // ç®€å•æµ‹è¯•
    setTimeout(async () => {
      try {
        const testUrl = `http://localhost:${port}/v1beta/models`;
        console.log(`ğŸ§ª æµ‹è¯•è¯·æ±‚: ${testUrl}`);

        const response = await fetch(testUrl);
        const data = await response.json();

        console.log('âœ… æµ‹è¯•æˆåŠŸ! æ¨¡å‹æ€»æ•°:', data.models.length);
        console.log('ğŸ‰ ä¼˜åŒ–åçš„MockServerè¿è¡Œå®Œå…¨æ­£å¸¸ï¼');
      } catch (error) {
        console.error('âŒ æµ‹è¯•å¤±è´¥:', error);
      }
    }, 1000);
  } catch (error) {
    console.error('âŒ MockServerå¯åŠ¨å¤±è´¥:', error);
  }
}
